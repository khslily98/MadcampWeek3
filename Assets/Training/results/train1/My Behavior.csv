Steps,Policy/Entropy,Policy/Extrinsic Value Estimate,Environment/Episode Length,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
50000,3.5475678,-2.2227488,289.3313253012048,1.4277770613510925,1.4277770613510925,0.40598446,0.025273068,0.0002838909,0.19463032,0.0047320523,1.0
100000,3.5084817,-1.0565948,274.96610169491527,1.538141201865875,1.538141201865875,0.12963377,0.024341816,0.00025579936,0.1852664,0.004264795,1.0
150000,3.4252331,-0.20697404,268.62702702702705,1.729210763082311,1.729210763082311,0.039467458,0.024184853,0.00022482772,0.17494258,0.003749634,1.0
200000,3.3438745,0.24283187,240.96172248803828,1.880808565997336,1.880808565997336,0.013451837,0.024377268,0.00019395893,0.16465297,0.0032361827,1.0
250000,3.2307403,0.44600263,243.6359223300971,2.1254285303829925,2.1254285303829925,0.009305803,0.02498761,0.0001631049,0.1543683,0.0027229772,1.0
300000,3.0792127,0.60012597,252.3041237113402,2.4161522690263495,2.4161522690263495,0.009282068,0.025754595,0.00013532616,0.1451087,0.002260924,1.0
350000,2.9790292,0.62248564,232.6082949308756,2.419649759058579,2.419649759058579,0.009135665,0.023906957,0.00010753904,0.13584633,0.001798731,1.0
400000,2.908753,0.6671832,245.64851485148515,2.503212837151962,2.503212837151962,0.00894873,0.021579985,7.671945e-05,0.12557313,0.0012860985,1.0
450000,2.8397524,0.6734925,252.7766497461929,2.5924517268759346,2.5924517268759346,0.009384921,0.024571884,4.5886405e-05,0.11529545,0.0007732424,1.0
500000,2.7784636,0.68710536,260.1216931216931,2.641941736457209,2.641941736457209,0.009980501,0.022621453,1.5018575e-05,0.10500616,0.0002598074,1.0

Steps,Policy/Entropy,Environment/Episode Length,Self-play/ELO,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
10000,3.5679655,495.90909090909093,1190.2164348062731,-0.3793556,-12.207425115257502,-12.207425115257502,0.20982121,0.07007564,0.00029999996,0.2,0.0049999994,1.0
20000,3.5488582,503.6666666666667,1172.9229261089442,-0.9843481,-14.241258103596536,-14.241258103596536,0.038961265,0.072862625,0.0003,0.20000005,0.0049999994,1.0
30000,3.5469928,498.5,1162.4245979258935,-1.1197534,-10.877911120653152,-10.877911120653152,0.024158986,0.061302852,0.00029999996,0.20000005,0.0049999994,1.0
40000,None,None,None,-1.2985424,None,None,None,None,None,None,None,1.0
50000,3.532949,None,None,-1.2662026,None,None,0.014851508,0.071274444,0.00029999999,0.20000002,0.005,1.0
60000,3.5328703,None,None,-1.344638,None,None,None,None,None,None,None,1.0
70000,3.5146525,640.0454545454545,1145.8336808803865,-1.2060964,-15.394856257840644,-15.394856257840644,0.02382563,0.07223988,0.00029999999,0.2,0.004999999,1.0
80000,3.4731703,793.125,1117.1708261654449,-1.1041093,-16.998657890793048,-16.998657890793048,0.01704971,0.07155521,0.0003,0.19999999,0.004999999,1.0
